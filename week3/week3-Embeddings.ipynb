{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "#download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "We will create a grader instace below and use it to collect your answers. Note that these outputs will be stored locally inside grader and will be uploaded to platform only after running submiting function in the last part of this assignment. If you want to make partial submission, you can run that cell any time you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grader import Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader = Grader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "                                                                 binary = True,\n",
    "                                                                limit = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    word_tokens = question.split(\" \")\n",
    "    question_len = len(word_tokens)\n",
    "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
    "    \n",
    "    for idx, word in enumerate(word_tokens):\n",
    "        if word in embeddings:\n",
    "            question_mat[idx,:] = embeddings[word]\n",
    "            \n",
    "    # remove zero-rows which stand for OOV words       \n",
    "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
    "    \n",
    "    # Compute the mean of each word along the sentence\n",
    "    if question_mat.shape[0] > 0:\n",
    "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
    "    else:\n",
    "        vec = np.zeros((1,dim), dtype = np.float32)\n",
    "        \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task Question2Vec is: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n"
     ]
    }
   ],
   "source": [
    "question2vec_result = []\n",
    "for question in open('data/test_embeddings.tsv'):\n",
    "    question = question.strip()\n",
    "    answer = question_to_vec(question, wv_embeddings)\n",
    "    question2vec_result = np.append(question2vec_result, answer)\n",
    "\n",
    "grader.submit_tag('Question2Vec', array_to_string(question2vec_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    return np.mean([1 if rank_dup <= k else 0 for rank_dup in dup_ranks])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    hitsk = np.array([1 if rank_dup <= k else 0 for rank_dup in dup_ranks])\n",
    "    \n",
    "    return np.mean(hitsk*1/(np.log2(1+np.array(dup_ranks))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    [1],\n",
    "    [1, 2],\n",
    "    [2, 1],\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    [9, 5, 4, 2, 8, 10, 7, 6, 1, 3],\n",
    "    [4, 3, 5, 1, 9, 10, 7, 8, 2, 6],\n",
    "    [5, 1, 7, 6, 2, 3, 8, 9, 10, 4],\n",
    "    [6, 3, 1, 4, 7, 2, 9, 8, 10, 5],\n",
    "    [10, 9, 8, 7, 6, 5, 4, 3, 2, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task HitsCount is: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0....\n"
     ]
    }
   ],
   "source": [
    "hits_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        hits_results.append(hits_count(example, k + 1))\n",
    "grader.submit_tag('HitsCount', array_to_string(hits_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task DCGScore is: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n"
     ]
    }
   ],
   "source": [
    "dcg_results = []\n",
    "for example in test_examples:\n",
    "    for k in range(len(example)):\n",
    "        dcg_results.append(dcg_score(example, k + 1))\n",
    "grader.submit_tag('DCGScore', array_to_string(dcg_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should upload *validation* corpus to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    # vectorize the embeddings of question and candidates\n",
    "    emb_question = np.array(question_to_vec(question,embeddings,dim))\n",
    "    emb_candidates = np.squeeze(np.array([question_to_vec(candidate,embeddings,dim) for candidate in candidates]))\n",
    "    # Compute the cosine similarity btw question and each candidate\n",
    "    cos_list = np.array(cosine_similarity(emb_question,emb_candidates)[0])\n",
    "    # Reshape results\n",
    "    pairs_unsorted = [(pos,candidates[pos],cos_list[pos]) for pos in range(len(candidates))]\n",
    "    pairs_sorted = sorted(pairs_unsorted,key = lambda x: x[2], reverse = True)\n",
    "    \n",
    "    return [(pos,cand) for pos,cand,_ in pairs_sorted]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.212 | Hits@   1: 0.212\n",
      "DCG@   5: 0.267 | Hits@   5: 0.315\n",
      "DCG@  10: 0.282 | Hits@  10: 0.363\n",
      "DCG@ 100: 0.320 | Hits@ 100: 0.552\n",
      "DCG@ 500: 0.353 | Hits@ 500: 0.811\n",
      "DCG@1000: 0.373 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    ######### YOUR CODE HERE #############\n",
    "    prepared_validation.append([text_prepare(sentence) for sentence in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.310 | Hits@   1: 0.310\n",
      "DCG@   5: 0.380 | Hits@   5: 0.443\n",
      "DCG@  10: 0.397 | Hits@  10: 0.494\n",
      "DCG@ 100: 0.430 | Hits@ 100: 0.661\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "prepare_file('data/train.tsv', 'data/train_processed.tsv')\n",
    "prepare_file('data/test.tsv', 'data/test_processed.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task W2VTokenizedRanks is: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n"
     ]
    }
   ],
   "source": [
    "w2v_ranks_results = []\n",
    "######### YOUR CODE HERE #############\n",
    "prepared_test_data = 'data/test_processed.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_processed.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_processed.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76.4%  lr: 0.042403  loss: 0.009783  eta: 0h21m  tot: 0h3m50s  (15.3%)%  lr: 0.049910  loss: 0.042342  eta: 0h28m  tot: 0h0m5s  (0.3%)2.3%  lr: 0.049810  loss: 0.038051  eta: 0h28m  tot: 0h0m8s  (0.5%)2.5%  lr: 0.049800  loss: 0.037303  eta: 0h28m  tot: 0h0m8s  (0.5%)3.1%  lr: 0.049750  loss: 0.034065  eta: 0h31m  tot: 0h0m11s  (0.6%)%  lr: 0.049740  loss: 0.033549  eta: 0h31m  tot: 0h0m11s  (0.6%)  lr: 0.049600  loss: 0.030223  eta: 0h30m  tot: 0h0m15s  (0.8%)4.3%  lr: 0.049570  loss: 0.029853  eta: 0h30m  tot: 0h0m15s  (0.9%)4.8%  lr: 0.049520  loss: 0.028725  eta: 0h29m  tot: 0h0m17s  (1.0%)5.1%  lr: 0.049489  loss: 0.027956  eta: 0h29m  tot: 0h0m18s  (1.0%)5.6%  lr: 0.049449  loss: 0.026724  eta: 0h28m  tot: 0h0m19s  (1.1%)5.7%  lr: 0.049439  loss: 0.026535  eta: 0h28m  tot: 0h0m19s  (1.1%)%  lr: 0.049409  loss: 0.026149  eta: 0h28m  tot: 0h0m21s  (1.2%)7.7%  lr: 0.049219  loss: 0.023944  eta: 0h27m  tot: 0h0m25s  (1.5%)7.9%  lr: 0.049219  loss: 0.023709  eta: 0h27m  tot: 0h0m26s  (1.6%)8.7%  lr: 0.049129  loss: 0.022724  eta: 0h26m  tot: 0h0m28s  (1.7%)9.7%  lr: 0.048989  loss: 0.021755  eta: 0h26m  tot: 0h0m31s  (1.9%)10.3%  lr: 0.048939  loss: 0.021190  eta: 0h26m  tot: 0h0m32s  (2.1%)11.7%  lr: 0.048839  loss: 0.020340  eta: 0h25m  tot: 0h0m36s  (2.3%)11.8%  lr: 0.048829  loss: 0.020278  eta: 0h25m  tot: 0h0m37s  (2.4%)13.2%  lr: 0.048679  loss: 0.019523  eta: 0h25m  tot: 0h0m41s  (2.6%)14.9%  lr: 0.048529  loss: 0.018741  eta: 0h24m  tot: 0h0m45s  (3.0%)15.2%  lr: 0.048479  loss: 0.018583  eta: 0h24m  tot: 0h0m46s  (3.0%)15.3%  lr: 0.048468  loss: 0.018525  eta: 0h24m  tot: 0h0m47s  (3.1%)15.4%  lr: 0.048458  loss: 0.018461  eta: 0h24m  tot: 0h0m47s  (3.1%)16.9%  lr: 0.048248  loss: 0.017876  eta: 0h24m  tot: 0h0m51s  (3.4%)17.6%  lr: 0.048168  loss: 0.017590  eta: 0h24m  tot: 0h0m53s  (3.5%)18.5%  lr: 0.048098  loss: 0.017214  eta: 0h24m  tot: 0h0m56s  (3.7%)18.5%  lr: 0.048088  loss: 0.017190  eta: 0h24m  tot: 0h0m56s  (3.7%)18.6%  lr: 0.048068  loss: 0.017188  eta: 0h24m  tot: 0h0m56s  (3.7%)18.7%  lr: 0.048068  loss: 0.017138  eta: 0h24m  tot: 0h0m56s  (3.7%)18.9%  lr: 0.048058  loss: 0.017077  eta: 0h24m  tot: 0h0m57s  (3.8%)19.4%  lr: 0.048028  loss: 0.016884  eta: 0h24m  tot: 0h0m58s  (3.9%)20.0%  lr: 0.047988  loss: 0.016682  eta: 0h24m  tot: 0h1m0s  (4.0%)20.2%  lr: 0.047978  loss: 0.016640  eta: 0h24m  tot: 0h1m1s  (4.0%)22.2%  lr: 0.047788  loss: 0.015949  eta: 0h24m  tot: 0h1m7s  (4.4%)22.7%  lr: 0.047738  loss: 0.015830  eta: 0h24m  tot: 0h1m9s  (4.5%)23.1%  lr: 0.047688  loss: 0.015716  eta: 0h24m  tot: 0h1m10s  (4.6%)23.5%  lr: 0.047648  loss: 0.015619  eta: 0h24m  tot: 0h1m11s  (4.7%)23.6%  lr: 0.047628  loss: 0.015584  eta: 0h24m  tot: 0h1m11s  (4.7%)23.8%  lr: 0.047608  loss: 0.015506  eta: 0h24m  tot: 0h1m12s  (4.8%)24.6%  lr: 0.047538  loss: 0.015311  eta: 0h24m  tot: 0h1m15s  (4.9%)25.5%  lr: 0.047468  loss: 0.015088  eta: 0h24m  tot: 0h1m19s  (5.1%)26.3%  lr: 0.047387  loss: 0.014862  eta: 0h24m  tot: 0h1m21s  (5.3%)27.0%  lr: 0.047337  loss: 0.014673  eta: 0h24m  tot: 0h1m23s  (5.4%)27.2%  lr: 0.047327  loss: 0.014619  eta: 0h24m  tot: 0h1m23s  (5.4%)%  lr: 0.047297  loss: 0.014582  eta: 0h24m  tot: 0h1m24s  (5.5%)28.1%  lr: 0.047217  loss: 0.014443  eta: 0h24m  tot: 0h1m27s  (5.6%)28.3%  lr: 0.047207  loss: 0.014438  eta: 0h24m  tot: 0h1m28s  (5.7%)28.5%  lr: 0.047187  loss: 0.014375  eta: 0h24m  tot: 0h1m29s  (5.7%)28.7%  lr: 0.047177  loss: 0.014332  eta: 0h24m  tot: 0h1m30s  (5.7%)30.0%  lr: 0.047097  loss: 0.014143  eta: 0h24m  tot: 0h1m34s  (6.0%)%  lr: 0.047087  loss: 0.014124  eta: 0h24m  tot: 0h1m34s  (6.0%)%  lr: 0.047027  loss: 0.013961  eta: 0h24m  tot: 0h1m37s  (6.2%)31.4%  lr: 0.046987  loss: 0.013878  eta: 0h24m  tot: 0h1m38s  (6.3%)32.1%  lr: 0.046947  loss: 0.013739  eta: 0h24m  tot: 0h1m40s  (6.4%)32.7%  lr: 0.046927  loss: 0.013622  eta: 0h24m  tot: 0h1m42s  (6.5%)33.1%  lr: 0.046907  loss: 0.013547  eta: 0h24m  tot: 0h1m43s  (6.6%)33.7%  lr: 0.046857  loss: 0.013432  eta: 0h24m  tot: 0h1m44s  (6.7%)34.1%  lr: 0.046807  loss: 0.013382  eta: 0h24m  tot: 0h1m46s  (6.8%)34.3%  lr: 0.046777  loss: 0.013362  eta: 0h24m  tot: 0h1m46s  (6.9%)%  lr: 0.046747  loss: 0.013344  eta: 0h24m  tot: 0h1m47s  (6.9%)34.9%  lr: 0.046717  loss: 0.013268  eta: 0h24m  tot: 0h1m48s  (7.0%)35.7%  lr: 0.046587  loss: 0.013168  eta: 0h24m  tot: 0h1m51s  (7.1%)37.4%  lr: 0.046416  loss: 0.012959  eta: 0h23m  tot: 0h1m56s  (7.5%)37.6%  lr: 0.046386  loss: 0.012929  eta: 0h23m  tot: 0h1m56s  (7.5%)  lr: 0.046376  loss: 0.012919  eta: 0h23m  tot: 0h1m57s  (7.5%)38.3%  lr: 0.046246  loss: 0.012865  eta: 0h23m  tot: 0h1m58s  (7.7%)38.7%  lr: 0.046216  loss: 0.012819  eta: 0h23m  tot: 0h1m59s  (7.7%)39.9%  lr: 0.046166  loss: 0.012657  eta: 0h23m  tot: 0h2m3s  (8.0%)40.1%  lr: 0.046126  loss: 0.012623  eta: 0h23m  tot: 0h2m3s  (8.0%)40.4%  lr: 0.046126  loss: 0.012571  eta: 0h23m  tot: 0h2m4s  (8.1%)41.2%  lr: 0.046036  loss: 0.012453  eta: 0h23m  tot: 0h2m6s  (8.2%)%  lr: 0.045896  loss: 0.012357  eta: 0h23m  tot: 0h2m9s  (8.4%)42.2%  lr: 0.045866  loss: 0.012339  eta: 0h23m  tot: 0h2m9s  (8.4%)42.3%  lr: 0.045866  loss: 0.012327  eta: 0h23m  tot: 0h2m9s  (8.5%)43.1%  lr: 0.045826  loss: 0.012250  eta: 0h23m  tot: 0h2m12s  (8.6%)43.2%  lr: 0.045826  loss: 0.012246  eta: 0h23m  tot: 0h2m12s  (8.6%)44.3%  lr: 0.045686  loss: 0.012154  eta: 0h23m  tot: 0h2m17s  (8.9%)44.7%  lr: 0.045676  loss: 0.012133  eta: 0h23m  tot: 0h2m19s  (8.9%)44.8%  lr: 0.045626  loss: 0.012109  eta: 0h23m  tot: 0h2m19s  (9.0%)45.6%  lr: 0.045576  loss: 0.012017  eta: 0h23m  tot: 0h2m22s  (9.1%)45.6%  lr: 0.045556  loss: 0.012009  eta: 0h23m  tot: 0h2m22s  (9.1%)46.9%  lr: 0.045436  loss: 0.011868  eta: 0h23m  tot: 0h2m26s  (9.4%)47.6%  lr: 0.045285  loss: 0.011804  eta: 0h23m  tot: 0h2m30s  (9.5%)%  lr: 0.045135  loss: 0.011664  eta: 0h23m  tot: 0h2m35s  (9.8%)49.2%  lr: 0.045135  loss: 0.011651  eta: 0h23m  tot: 0h2m35s  (9.8%)52.7%  lr: 0.044825  loss: 0.011362  eta: 0h23m  tot: 0h2m45s  (10.5%)52.9%  lr: 0.044805  loss: 0.011333  eta: 0h23m  tot: 0h2m46s  (10.6%)53.9%  lr: 0.044695  loss: 0.011241  eta: 0h23m  tot: 0h2m49s  (10.8%)54.1%  lr: 0.044685  loss: 0.011224  eta: 0h23m  tot: 0h2m49s  (10.8%)54.6%  lr: 0.044675  loss: 0.011181  eta: 0h23m  tot: 0h2m50s  (10.9%)%  lr: 0.044655  loss: 0.011173  eta: 0h23m  tot: 0h2m51s  (10.9%)55.3%  lr: 0.044555  loss: 0.011139  eta: 0h23m  tot: 0h2m52s  (11.1%)55.7%  lr: 0.044515  loss: 0.011124  eta: 0h23m  tot: 0h2m54s  (11.1%)56.0%  lr: 0.044505  loss: 0.011098  eta: 0h23m  tot: 0h2m54s  (11.2%)56.6%  lr: 0.044475  loss: 0.011054  eta: 0h23m  tot: 0h2m56s  (11.3%)57.3%  lr: 0.044364  loss: 0.011019  eta: 0h22m  tot: 0h2m58s  (11.5%)59.7%  lr: 0.044114  loss: 0.010812  eta: 0h22m  tot: 0h3m5s  (11.9%)60.0%  lr: 0.044104  loss: 0.010785  eta: 0h22m  tot: 0h3m5s  (12.0%)60.0%  lr: 0.044104  loss: 0.010786  eta: 0h22m  tot: 0h3m6s  (12.0%)60.3%  lr: 0.044034  loss: 0.010768  eta: 0h22m  tot: 0h3m6s  (12.1%)61.2%  lr: 0.043894  loss: 0.010695  eta: 0h22m  tot: 0h3m9s  (12.2%)63.5%  lr: 0.043654  loss: 0.010539  eta: 0h22m  tot: 0h3m15s  (12.7%)65.7%  lr: 0.043404  loss: 0.010388  eta: 0h22m  tot: 0h3m21s  (13.1%)66.0%  lr: 0.043363  loss: 0.010375  eta: 0h22m  tot: 0h3m22s  (13.2%)66.5%  lr: 0.043283  loss: 0.010335  eta: 0h22m  tot: 0h3m23s  (13.3%)66.6%  lr: 0.043283  loss: 0.010332  eta: 0h22m  tot: 0h3m24s  (13.3%)66.9%  lr: 0.043253  loss: 0.010311  eta: 0h22m  tot: 0h3m24s  (13.4%)68.1%  lr: 0.043123  loss: 0.010238  eta: 0h21m  tot: 0h3m27s  (13.6%)69.3%  lr: 0.043033  loss: 0.010162  eta: 0h21m  tot: 0h3m31s  (13.9%)70.2%  lr: 0.042963  loss: 0.010105  eta: 0h21m  tot: 0h3m33s  (14.0%)71.7%  lr: 0.042823  loss: 0.010028  eta: 0h21m  tot: 0h3m37s  (14.3%)72.4%  lr: 0.042783  loss: 0.009985  eta: 0h21m  tot: 0h3m39s  (14.5%)72.6%  lr: 0.042763  loss: 0.009977  eta: 0h21m  tot: 0h3m40s  (14.5%)73.5%  lr: 0.042633  loss: 0.009923  eta: 0h21m  tot: 0h3m42s  (14.7%)73.6%  lr: 0.042633  loss: 0.009927  eta: 0h21m  tot: 0h3m43s  (14.7%)75.0%  lr: 0.042553  loss: 0.009852  eta: 0h21m  tot: 0h3m46s  (15.0%)%  lr: 0.042453  loss: 0.009807  eta: 0h21m  tot: 0h3m49s  (15.2%)76.5%  lr: 0.042393  loss: 0.009779  eta: 0h21m  tot: 0h3m50s  (15.3%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.040000  loss: 0.008916  eta: 0h19m  tot: 0h4m55s  (20.0%)6.8%  lr: 0.042332  loss: 0.009765  eta: 0h21m  tot: 0h3m51s  (15.4%)%  lr: 0.042262  loss: 0.009744  eta: 0h21m  tot: 0h3m53s  (15.5%)77.8%  lr: 0.042222  loss: 0.009727  eta: 0h21m  tot: 0h3m54s  (15.6%)79.2%  lr: 0.042062  loss: 0.009658  eta: 0h21m  tot: 0h3m58s  (15.8%)80.8%  lr: 0.041922  loss: 0.009606  eta: 0h20m  tot: 0h4m2s  (16.2%)81.5%  lr: 0.041882  loss: 0.009578  eta: 0h20m  tot: 0h4m4s  (16.3%)81.7%  lr: 0.041852  loss: 0.009566  eta: 0h20m  tot: 0h4m5s  (16.3%)82.0%  lr: 0.041822  loss: 0.009558  eta: 0h20m  tot: 0h4m5s  (16.4%)%  lr: 0.041772  loss: 0.009541  eta: 0h20m  tot: 0h4m6s  (16.5%)82.6%  lr: 0.041732  loss: 0.009534  eta: 0h20m  tot: 0h4m7s  (16.5%)%  lr: 0.041612  loss: 0.009505  eta: 0h20m  tot: 0h4m9s  (16.7%)84.1%  lr: 0.041552  loss: 0.009475  eta: 0h20m  tot: 0h4m11s  (16.8%)84.4%  lr: 0.041472  loss: 0.009457  eta: 0h20m  tot: 0h4m12s  (16.9%)%  lr: 0.041462  loss: 0.009454  eta: 0h20m  tot: 0h4m12s  (16.9%)85.2%  lr: 0.041311  loss: 0.009434  eta: 0h20m  tot: 0h4m14s  (17.0%)85.5%  lr: 0.041271  loss: 0.009427  eta: 0h20m  tot: 0h4m15s  (17.1%)85.7%  lr: 0.041251  loss: 0.009423  eta: 0h20m  tot: 0h4m15s  (17.1%)86.7%  lr: 0.041141  loss: 0.009386  eta: 0h20m  tot: 0h4m18s  (17.3%)86.8%  lr: 0.041141  loss: 0.009387  eta: 0h20m  tot: 0h4m18s  (17.4%)87.1%  lr: 0.041101  loss: 0.009367  eta: 0h20m  tot: 0h4m19s  (17.4%)87.3%  lr: 0.041091  loss: 0.009363  eta: 0h20m  tot: 0h4m20s  (17.5%)87.8%  lr: 0.041061  loss: 0.009348  eta: 0h20m  tot: 0h4m21s  (17.6%)88.0%  lr: 0.040981  loss: 0.009333  eta: 0h20m  tot: 0h4m22s  (17.6%)89.4%  lr: 0.040841  loss: 0.009282  eta: 0h20m  tot: 0h4m26s  (17.9%)89.5%  lr: 0.040821  loss: 0.009275  eta: 0h20m  tot: 0h4m26s  (17.9%)89.6%  lr: 0.040811  loss: 0.009270  eta: 0h20m  tot: 0h4m26s  (17.9%)%  lr: 0.040781  loss: 0.009252  eta: 0h20m  tot: 0h4m28s  (18.0%)90.7%  lr: 0.040721  loss: 0.009250  eta: 0h20m  tot: 0h4m30s  (18.1%)90.7%  lr: 0.040691  loss: 0.009246  eta: 0h20m  tot: 0h4m30s  (18.1%)91.3%  lr: 0.040611  loss: 0.009226  eta: 0h20m  tot: 0h4m32s  (18.3%)91.5%  lr: 0.040601  loss: 0.009219  eta: 0h20m  tot: 0h4m33s  (18.3%)91.7%  lr: 0.040571  loss: 0.009214  eta: 0h20m  tot: 0h4m33s  (18.3%)91.9%  lr: 0.040571  loss: 0.009211  eta: 0h20m  tot: 0h4m34s  (18.4%)92.0%  lr: 0.040561  loss: 0.009210  eta: 0h20m  tot: 0h4m35s  (18.4%)93.8%  lr: 0.040411  loss: 0.009132  eta: 0h20m  tot: 0h4m40s  (18.8%)94.7%  lr: 0.040320  loss: 0.009098  eta: 0h20m  tot: 0h4m43s  (18.9%)95.2%  lr: 0.040260  loss: 0.009077  eta: 0h20m  tot: 0h4m44s  (19.0%)95.3%  lr: 0.040250  loss: 0.009071  eta: 0h20m  tot: 0h4m45s  (19.1%)95.7%  lr: 0.040210  loss: 0.009060  eta: 0h20m  tot: 0h4m46s  (19.1%)95.9%  lr: 0.040210  loss: 0.009052  eta: 0h20m  tot: 0h4m46s  (19.2%)96.4%  lr: 0.040170  loss: 0.009037  eta: 0h20m  tot: 0h4m48s  (19.3%)20m  tot: 0h4m49s  (19.3%)96.9%  lr: 0.040110  loss: 0.009028  eta: 0h20m  tot: 0h4m50s  (19.4%)\n",
      " ---+++                Epoch    0 Train error : 0.00891565 +++--- ���\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 90.7%  lr: 0.030611  loss: 0.002700  eta: 0h16m  tot: 0h9m44s  (38.1%).4%  lr: 0.039760  loss: 0.002746  eta: 0h18m  tot: 0h5m2s  (20.5%)4.1%  lr: 0.039600  loss: 0.002565  eta: 0h20m  tot: 0h5m8s  (20.8%)4.6%  lr: 0.039550  loss: 0.002545  eta: 0h21m  tot: 0h5m10s  (20.9%)5.1%  lr: 0.039479  loss: 0.002506  eta: 0h21m  tot: 0h5m12s  (21.0%)5.3%  lr: 0.039469  loss: 0.002487  eta: 0h21m  tot: 0h5m13s  (21.1%)5.4%  lr: 0.039459  loss: 0.002482  eta: 0h21m  tot: 0h5m13s  (21.1%)6.4%  lr: 0.039299  loss: 0.002620  eta: 0h21m  tot: 0h5m16s  (21.3%)8.4%  lr: 0.039089  loss: 0.002646  eta: 0h20m  tot: 0h5m22s  (21.7%)8.9%  lr: 0.039039  loss: 0.002700  eta: 0h20m  tot: 0h5m23s  (21.8%)9.0%  lr: 0.039039  loss: 0.002704  eta: 0h20m  tot: 0h5m23s  (21.8%)10.0%  lr: 0.038959  loss: 0.002735  eta: 0h20m  tot: 0h5m26s  (22.0%)10.1%  lr: 0.038929  loss: 0.002726  eta: 0h20m  tot: 0h5m26s  (22.0%)10.4%  lr: 0.038879  loss: 0.002718  eta: 0h20m  tot: 0h5m27s  (22.1%)10.4%  lr: 0.038849  loss: 0.002716  eta: 0h20m  tot: 0h5m28s  (22.1%)11.7%  lr: 0.038689  loss: 0.002682  eta: 0h19m  tot: 0h5m31s  (22.3%)%  lr: 0.038689  loss: 0.002670  eta: 0h19m  tot: 0h5m31s  (22.4%)12.0%  lr: 0.038659  loss: 0.002653  eta: 0h19m  tot: 0h5m32s  (22.4%)12.4%  lr: 0.038589  loss: 0.002609  eta: 0h19m  tot: 0h5m33s  (22.5%)12.5%  lr: 0.038579  loss: 0.002602  eta: 0h19m  tot: 0h5m34s  (22.5%)12.9%  lr: 0.038549  loss: 0.002626  eta: 0h19m  tot: 0h5m35s  (22.6%)13.1%  lr: 0.038519  loss: 0.002611  eta: 0h19m  tot: 0h5m35s  (22.6%)14.6%  lr: 0.038388  loss: 0.002565  eta: 0h19m  tot: 0h5m39s  (22.9%)15.4%  lr: 0.038328  loss: 0.002572  eta: 0h19m  tot: 0h5m42s  (23.1%)15.8%  lr: 0.038278  loss: 0.002570  eta: 0h19m  tot: 0h5m43s  (23.2%)16.1%  lr: 0.038248  loss: 0.002564  eta: 0h19m  tot: 0h5m44s  (23.2%)16.8%  lr: 0.038148  loss: 0.002603  eta: 0h19m  tot: 0h5m46s  (23.4%)17.0%  lr: 0.038108  loss: 0.002604  eta: 0h19m  tot: 0h5m47s  (23.4%)18.6%  lr: 0.037908  loss: 0.002673  eta: 0h19m  tot: 0h5m52s  (23.7%)21.3%  lr: 0.037688  loss: 0.002669  eta: 0h19m  tot: 0h6m1s  (24.3%)22.9%  lr: 0.037457  loss: 0.002697  eta: 0h19m  tot: 0h6m6s  (24.6%)%  lr: 0.037447  loss: 0.002693  eta: 0h19m  tot: 0h6m6s  (24.6%)23.2%  lr: 0.037427  loss: 0.002700  eta: 0h19m  tot: 0h6m7s  (24.6%)23.9%  lr: 0.037307  loss: 0.002693  eta: 0h19m  tot: 0h6m9s  (24.8%)24.7%  lr: 0.037197  loss: 0.002701  eta: 0h19m  tot: 0h6m12s  (24.9%)24.8%  lr: 0.037197  loss: 0.002697  eta: 0h19m  tot: 0h6m12s  (25.0%)24.8%  lr: 0.037197  loss: 0.002696  eta: 0h19m  tot: 0h6m13s  (25.0%)%  lr: 0.037157  loss: 0.002694  eta: 0h19m  tot: 0h6m14s  (25.0%)25.4%  lr: 0.037157  loss: 0.002686  eta: 0h19m  tot: 0h6m14s  (25.1%)25.8%  lr: 0.037097  loss: 0.002685  eta: 0h19m  tot: 0h6m16s  (25.2%)27.8%  lr: 0.036947  loss: 0.002673  eta: 0h19m  tot: 0h6m21s  (25.6%)28.4%  lr: 0.036907  loss: 0.002685  eta: 0h19m  tot: 0h6m23s  (25.7%)%  lr: 0.036837  loss: 0.002671  eta: 0h19m  tot: 0h6m24s  (25.7%)28.9%  lr: 0.036827  loss: 0.002681  eta: 0h19m  tot: 0h6m25s  (25.8%)30.7%  lr: 0.036647  loss: 0.002675  eta: 0h18m  tot: 0h6m30s  (26.1%)31.6%  lr: 0.036577  loss: 0.002686  eta: 0h18m  tot: 0h6m33s  (26.3%)31.7%  lr: 0.036577  loss: 0.002685  eta: 0h18m  tot: 0h6m33s  (26.3%)32.2%  lr: 0.036557  loss: 0.002680  eta: 0h18m  tot: 0h6m35s  (26.4%)33.6%  lr: 0.036436  loss: 0.002684  eta: 0h18m  tot: 0h6m39s  (26.7%)33.8%  lr: 0.036426  loss: 0.002689  eta: 0h18m  tot: 0h6m40s  (26.8%)6m43s  (27.0%)35.3%  lr: 0.036226  loss: 0.002675  eta: 0h18m  tot: 0h6m45s  (27.1%)35.7%  lr: 0.036146  loss: 0.002678  eta: 0h18m  tot: 0h6m46s  (27.1%)36.0%  lr: 0.036136  loss: 0.002674  eta: 0h18m  tot: 0h6m47s  (27.2%)36.5%  lr: 0.036096  loss: 0.002673  eta: 0h18m  tot: 0h6m48s  (27.3%)36.6%  lr: 0.036096  loss: 0.002676  eta: 0h18m  tot: 0h6m49s  (27.3%)36.8%  lr: 0.036086  loss: 0.002678  eta: 0h18m  tot: 0h6m49s  (27.4%)36.9%  lr: 0.036086  loss: 0.002684  eta: 0h18m  tot: 0h6m50s  (27.4%)37.1%  lr: 0.036086  loss: 0.002695  eta: 0h18m  tot: 0h6m50s  (27.4%)38.1%  lr: 0.036006  loss: 0.002703  eta: 0h18m  tot: 0h6m54s  (27.6%)40.2%  lr: 0.035796  loss: 0.002698  eta: 0h18m  tot: 0h7m2s  (28.0%)43.2%  lr: 0.035546  loss: 0.002724  eta: 0h18m  tot: 0h7m11s  (28.6%)43.6%  lr: 0.035516  loss: 0.002715  eta: 0h18m  tot: 0h7m12s  (28.7%)43.9%  lr: 0.035476  loss: 0.002717  eta: 0h18m  tot: 0h7m13s  (28.8%)44.8%  lr: 0.035385  loss: 0.002704  eta: 0h18m  tot: 0h7m17s  (29.0%)45.2%  lr: 0.035355  loss: 0.002698  eta: 0h18m  tot: 0h7m18s  (29.0%)46.1%  lr: 0.035265  loss: 0.002718  eta: 0h18m  tot: 0h7m21s  (29.2%)46.5%  lr: 0.035185  loss: 0.002719  eta: 0h18m  tot: 0h7m22s  (29.3%)46.9%  lr: 0.035125  loss: 0.002715  eta: 0h18m  tot: 0h7m23s  (29.4%)47.9%  lr: 0.034995  loss: 0.002717  eta: 0h18m  tot: 0h7m27s  (29.6%)49.1%  lr: 0.034905  loss: 0.002730  eta: 0h18m  tot: 0h7m30s  (29.8%)49.9%  lr: 0.034815  loss: 0.002721  eta: 0h18m  tot: 0h7m33s  (30.0%)50.0%  lr: 0.034815  loss: 0.002722  eta: 0h18m  tot: 0h7m33s  (30.0%)50.6%  lr: 0.034745  loss: 0.002723  eta: 0h18m  tot: 0h7m35s  (30.1%)50.9%  lr: 0.034735  loss: 0.002724  eta: 0h18m  tot: 0h7m36s  (30.2%)53.2%  lr: 0.034565  loss: 0.002703  eta: 0h18m  tot: 0h7m42s  (30.6%)%  lr: 0.034555  loss: 0.002702  eta: 0h18m  tot: 0h7m42s  (30.7%)54.1%  lr: 0.034435  loss: 0.002711  eta: 0h18m  tot: 0h7m45s  (30.8%)54.8%  lr: 0.034384  loss: 0.002711  eta: 0h17m  tot: 0h7m47s  (31.0%)55.3%  lr: 0.034344  loss: 0.002709  eta: 0h17m  tot: 0h7m48s  (31.1%)56.7%  lr: 0.034184  loss: 0.002703  eta: 0h17m  tot: 0h7m52s  (31.3%)  loss: 0.002697  eta: 0h17m  tot: 0h7m55s  (31.5%)58.0%  lr: 0.034094  loss: 0.002700  eta: 0h17m  tot: 0h7m56s  (31.6%)58.5%  lr: 0.034074  loss: 0.002700  eta: 0h17m  tot: 0h7m58s  (31.7%)60.9%  lr: 0.033824  loss: 0.002698  eta: 0h17m  tot: 0h8m6s  (32.2%)61.6%  lr: 0.033734  loss: 0.002696  eta: 0h17m  tot: 0h8m9s  (32.3%)61.7%  lr: 0.033724  loss: 0.002694  eta: 0h17m  tot: 0h8m9s  (32.3%)62.7%  lr: 0.033604  loss: 0.002683  eta: 0h17m  tot: 0h8m12s  (32.5%)63.2%  lr: 0.033564  loss: 0.002683  eta: 0h17m  tot: 0h8m14s  (32.6%)63.4%  lr: 0.033514  loss: 0.002681  eta: 0h17m  tot: 0h8m15s  (32.7%)64.7%  lr: 0.033383  loss: 0.002686  eta: 0h17m  tot: 0h8m19s  (32.9%)20s  (33.0%)65.0%  lr: 0.033343  loss: 0.002680  eta: 0h17m  tot: 0h8m20s  (33.0%)  lr: 0.033103  loss: 0.002676  eta: 0h17m  tot: 0h8m30s  (33.6%)68.0%  lr: 0.033093  loss: 0.002677  eta: 0h17m  tot: 0h8m30s  (33.6%)68.2%  lr: 0.033063  loss: 0.002678  eta: 0h17m  tot: 0h8m31s  (33.6%)68.3%  lr: 0.033053  loss: 0.002676  eta: 0h17m  tot: 0h8m32s  (33.7%)70.0%  lr: 0.032953  loss: 0.002674  eta: 0h17m  tot: 0h8m38s  (34.0%)70.2%  lr: 0.032933  loss: 0.002680  eta: 0h17m  tot: 0h8m38s  (34.0%)70.3%  lr: 0.032923  loss: 0.002680  eta: 0h17m  tot: 0h8m39s  (34.1%)71.7%  lr: 0.032713  loss: 0.002680  eta: 0h17m  tot: 0h8m43s  (34.3%)73.3%  lr: 0.032493  loss: 0.002684  eta: 0h17m  tot: 0h8m48s  (34.7%)%  lr: 0.032453  loss: 0.002685  eta: 0h17m  tot: 0h8m49s  (34.7%)75.0%  lr: 0.032332  loss: 0.002689  eta: 0h17m  tot: 0h8m53s  (35.0%)75.7%  lr: 0.032192  loss: 0.002689  eta: 0h17m  tot: 0h8m55s  (35.1%)76.1%  lr: 0.032172  loss: 0.002691  eta: 0h17m  tot: 0h8m56s  (35.2%)76.7%  lr: 0.032112  loss: 0.002687  eta: 0h17m  tot: 0h8m58s  (35.3%)%  lr: 0.031982  loss: 0.002700  eta: 0h17m  tot: 0h9m3s  (35.6%)%  lr: 0.031902  loss: 0.002699  eta: 0h17m  tot: 0h9m4s  (35.6%)%  lr: 0.031742  loss: 0.002713  eta: 0h16m  tot: 0h9m7s  (35.9%)79.8%  lr: 0.031702  loss: 0.002715  eta: 0h16m  tot: 0h9m9s  (36.0%)80.5%  lr: 0.031642  loss: 0.002713  eta: 0h16m  tot: 0h9m11s  (36.1%)80.8%  lr: 0.031622  loss: 0.002712  eta: 0h16m  tot: 0h9m11s  (36.2%)80.8%  lr: 0.031612  loss: 0.002712  eta: 0h16m  tot: 0h9m12s  (36.2%)  lr: 0.031512  loss: 0.002719  eta: 0h16m  tot: 0h9m16s  (36.5%)82.8%  lr: 0.031502  loss: 0.002714  eta: 0h16m  tot: 0h9m18s  (36.6%)84.0%  lr: 0.031372  loss: 0.002706  eta: 0h16m  tot: 0h9m21s  (36.8%)84.8%  lr: 0.031281  loss: 0.002703  eta: 0h16m  tot: 0h9m24s  (37.0%)84.9%  lr: 0.031271  loss: 0.002701  eta: 0h16m  tot: 0h9m24s  (37.0%)90.8%  lr: 0.030591  loss: 0.002702  eta: 0h16m  tot: 0h9m44s  (38.2%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.002688  eta: 0h15m  tot: 0h10m5s  (40.0%)1.0%  lr: 0.030561  loss: 0.002703  eta: 0h16m  tot: 0h9m45s  (38.2%)91.4%  lr: 0.030521  loss: 0.002706  eta: 0h16m  tot: 0h9m46s  (38.3%)91.6%  lr: 0.030511  loss: 0.002705  eta: 0h16m  tot: 0h9m47s  (38.3%)93.1%  lr: 0.030391  loss: 0.002696  eta: 0h16m  tot: 0h9m51s  (38.6%)94.6%  lr: 0.030230  loss: 0.002692  eta: 0h16m  tot: 0h9m55s  (38.9%)94.7%  lr: 0.030230  loss: 0.002691  eta: 0h16m  tot: 0h9m56s  (38.9%)95.1%  lr: 0.030190  loss: 0.002690  eta: 0h16m  tot: 0h9m57s  (39.0%)95.2%  lr: 0.030190  loss: 0.002688  eta: 0h16m  tot: 0h9m57s  (39.0%)95.9%  lr: 0.030150  loss: 0.002688  eta: 0h16m  tot: 0h9m59s  (39.2%)96.3%  lr: 0.030100  loss: 0.002685  eta: 0h16m  tot: 0h10m0s  (39.3%)97.2%  lr: 0.030050  loss: 0.002685  eta: 0h15m  tot: 0h10m3s  (39.4%)97.4%  lr: 0.030040  loss: 0.002683  eta: 0h15m  tot: 0h10m3s  (39.5%)98.3%  lr: 0.030010  loss: 0.002687  eta: 0h15m  tot: 0h10m4s  (39.7%)\n",
      " ---+++                Epoch    1 Train error : 0.00267039 +++--- ���\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 72.0%  lr: 0.022663  loss: 0.001869  eta: 0h11m  tot: 0h13m36s  (54.4%)5%  lr: 0.029930  loss: 0.002954  eta: 0h12m  tot: 0h10m7s  (40.1%)%  lr: 0.029930  loss: 0.002708  eta: 0h12m  tot: 0h10m7s  (40.1%)0.7%  lr: 0.029920  loss: 0.002591  eta: 0h12m  tot: 0h10m7s  (40.1%)1.0%  lr: 0.029880  loss: 0.002290  eta: 0h12m  tot: 0h10m8s  (40.2%)2.1%  lr: 0.029780  loss: 0.002149  eta: 0h15m  tot: 0h10m11s  (40.4%)2.3%  lr: 0.029730  loss: 0.002034  eta: 0h15m  tot: 0h10m12s  (40.5%)2.4%  lr: 0.029710  loss: 0.002031  eta: 0h15m  tot: 0h10m13s  (40.5%)3.4%  lr: 0.029610  loss: 0.002041  eta: 0h15m  tot: 0h10m16s  (40.7%)3.6%  lr: 0.029610  loss: 0.002000  eta: 0h15m  tot: 0h10m16s  (40.7%)%  lr: 0.029570  loss: 0.001978  eta: 0h15m  tot: 0h10m17s  (40.8%)4.2%  lr: 0.029520  loss: 0.001995  eta: 0h15m  tot: 0h10m18s  (40.8%)4.5%  lr: 0.029489  loss: 0.002027  eta: 0h15m  tot: 0h10m19s  (40.9%)4.6%  lr: 0.029489  loss: 0.002016  eta: 0h15m  tot: 0h10m19s  (40.9%)%  lr: 0.029489  loss: 0.001984  eta: 0h15m  tot: 0h10m20s  (40.9%)4.9%  lr: 0.029439  loss: 0.001997  eta: 0h15m  tot: 0h10m20s  (41.0%)5.0%  lr: 0.029429  loss: 0.001976  eta: 0h15m  tot: 0h10m20s  (41.0%)5.6%  lr: 0.029379  loss: 0.001910  eta: 0h15m  tot: 0h10m22s  (41.1%)6.9%  lr: 0.029239  loss: 0.001903  eta: 0h14m  tot: 0h10m26s  (41.4%)8.7%  lr: 0.029019  loss: 0.001933  eta: 0h14m  tot: 0h10m32s  (41.7%)9.0%  lr: 0.028959  loss: 0.001938  eta: 0h14m  tot: 0h10m33s  (41.8%)9.2%  lr: 0.028929  loss: 0.001944  eta: 0h14m  tot: 0h10m33s  (41.8%)9.5%  lr: 0.028879  loss: 0.001929  eta: 0h14m  tot: 0h10m34s  (41.9%)10.3%  lr: 0.028789  loss: 0.001991  eta: 0h14m  tot: 0h10m36s  (42.1%)11.5%  lr: 0.028539  loss: 0.001967  eta: 0h14m  tot: 0h10m40s  (42.3%)11.7%  lr: 0.028519  loss: 0.001960  eta: 0h14m  tot: 0h10m41s  (42.3%)%  lr: 0.028509  loss: 0.001953  eta: 0h14m  tot: 0h10m41s  (42.4%)12.2%  lr: 0.028468  loss: 0.001942  eta: 0h14m  tot: 0h10m42s  (42.4%)12.3%  lr: 0.028458  loss: 0.001940  eta: 0h14m  tot: 0h10m43s  (42.5%)13.1%  lr: 0.028388  loss: 0.001918  eta: 0h14m  tot: 0h10m45s  (42.6%)14.2%  lr: 0.028288  loss: 0.001897  eta: 0h14m  tot: 0h10m48s  (42.8%)14.9%  lr: 0.028198  loss: 0.001902  eta: 0h14m  tot: 0h10m50s  (43.0%)15.2%  lr: 0.028148  loss: 0.001912  eta: 0h14m  tot: 0h10m51s  (43.0%)16.0%  lr: 0.028048  loss: 0.001928  eta: 0h14m  tot: 0h10m53s  (43.2%)16.1%  lr: 0.028038  loss: 0.001923  eta: 0h14m  tot: 0h10m54s  (43.2%)16.7%  lr: 0.027988  loss: 0.001922  eta: 0h14m  tot: 0h10m55s  (43.3%)17.0%  lr: 0.027968  loss: 0.001913  eta: 0h14m  tot: 0h10m56s  (43.4%)18.0%  lr: 0.027888  loss: 0.001904  eta: 0h14m  tot: 0h10m59s  (43.6%)19.4%  lr: 0.027708  loss: 0.001920  eta: 0h14m  tot: 0h11m4s  (43.9%)21.3%  lr: 0.027488  loss: 0.001893  eta: 0h13m  tot: 0h11m9s  (44.3%)21.5%  lr: 0.027457  loss: 0.001902  eta: 0h13m  tot: 0h11m10s  (44.3%)22.1%  lr: 0.027387  loss: 0.001897  eta: 0h13m  tot: 0h11m11s  (44.4%)%  lr: 0.027357  loss: 0.001903  eta: 0h13m  tot: 0h11m13s  (44.5%)23.0%  lr: 0.027277  loss: 0.001908  eta: 0h13m  tot: 0h11m14s  (44.6%)23.8%  lr: 0.027177  loss: 0.001907  eta: 0h13m  tot: 0h11m17s  (44.8%)%  lr: 0.027167  loss: 0.001906  eta: 0h13m  tot: 0h11m18s  (44.8%)26.4%  lr: 0.026947  loss: 0.001916  eta: 0h13m  tot: 0h11m24s  (45.3%)26.6%  lr: 0.026937  loss: 0.001926  eta: 0h13m  tot: 0h11m25s  (45.3%)27.2%  lr: 0.026857  loss: 0.001917  eta: 0h13m  tot: 0h11m27s  (45.4%)27.7%  lr: 0.026797  loss: 0.001914  eta: 0h13m  tot: 0h11m28s  (45.5%)28.7%  lr: 0.026697  loss: 0.001916  eta: 0h13m  tot: 0h11m31s  (45.7%)29.1%  lr: 0.026667  loss: 0.001914  eta: 0h13m  tot: 0h11m32s  (45.8%)30.4%  lr: 0.026567  loss: 0.001893  eta: 0h13m  tot: 0h11m36s  (46.1%)30.5%  lr: 0.026567  loss: 0.001893  eta: 0h13m  tot: 0h11m36s  (46.1%)31.2%  lr: 0.026447  loss: 0.001896  eta: 0h13m  tot: 0h11m38s  (46.2%)31.6%  lr: 0.026386  loss: 0.001898  eta: 0h13m  tot: 0h11m39s  (46.3%)31.9%  lr: 0.026356  loss: 0.001901  eta: 0h13m  tot: 0h11m40s  (46.4%)32.1%  lr: 0.026336  loss: 0.001900  eta: 0h13m  tot: 0h11m41s  (46.4%)32.6%  lr: 0.026286  loss: 0.001899  eta: 0h13m  tot: 0h11m42s  (46.5%)%  lr: 0.026246  loss: 0.001907  eta: 0h13m  tot: 0h11m43s  (46.6%)33.5%  lr: 0.026196  loss: 0.001913  eta: 0h13m  tot: 0h11m44s  (46.7%)33.7%  lr: 0.026176  loss: 0.001917  eta: 0h13m  tot: 0h11m45s  (46.7%)35.2%  lr: 0.026066  loss: 0.001903  eta: 0h13m  tot: 0h11m49s  (47.0%)37.0%  lr: 0.025926  loss: 0.001906  eta: 0h13m  tot: 0h11m56s  (47.4%)37.5%  lr: 0.025896  loss: 0.001904  eta: 0h13m  tot: 0h11m57s  (47.5%)38.3%  lr: 0.025846  loss: 0.001899  eta: 0h13m  tot: 0h12m0s  (47.7%)39.6%  lr: 0.025696  loss: 0.001889  eta: 0h12m  tot: 0h12m4s  (47.9%)39.8%  lr: 0.025676  loss: 0.001892  eta: 0h12m  tot: 0h12m4s  (48.0%)41.0%  lr: 0.025576  loss: 0.001892  eta: 0h12m  tot: 0h12m8s  (48.2%)%  lr: 0.025556  loss: 0.001890  eta: 0h12m  tot: 0h12m8s  (48.2%)  loss: 0.001887  eta: 0h12m  tot: 0h12m10s  (48.3%)41.8%  lr: 0.025506  loss: 0.001886  eta: 0h12m  tot: 0h12m10s  (48.4%)%  lr: 0.025496  loss: 0.001886  eta: 0h12m  tot: 0h12m10s  (48.4%)42.8%  lr: 0.025405  loss: 0.001878  eta: 0h12m  tot: 0h12m13s  (48.6%)43.1%  lr: 0.025365  loss: 0.001875  eta: 0h12m  tot: 0h12m14s  (48.6%)43.2%  lr: 0.025365  loss: 0.001872  eta: 0h12m  tot: 0h12m14s  (48.6%)%  lr: 0.025355  loss: 0.001871  eta: 0h12m  tot: 0h12m14s  (48.7%)44.7%  lr: 0.025185  loss: 0.001872  eta: 0h12m  tot: 0h12m18s  (48.9%)45.3%  lr: 0.025125  loss: 0.001871  eta: 0h12m  tot: 0h12m20s  (49.1%)45.8%  lr: 0.025035  loss: 0.001865  eta: 0h12m  tot: 0h12m21s  (49.2%)46.3%  lr: 0.025015  loss: 0.001865  eta: 0h12m  tot: 0h12m23s  (49.3%)46.8%  lr: 0.024965  loss: 0.001866  eta: 0h12m  tot: 0h12m24s  (49.4%)46.9%  lr: 0.024965  loss: 0.001866  eta: 0h12m  tot: 0h12m24s  (49.4%)47.0%  lr: 0.024955  loss: 0.001867  eta: 0h12m  tot: 0h12m25s  (49.4%)47.1%  lr: 0.024935  loss: 0.001866  eta: 0h12m  tot: 0h12m25s  (49.4%)47.4%  lr: 0.024895  loss: 0.001875  eta: 0h12m  tot: 0h12m26s  (49.5%)48.3%  lr: 0.024825  loss: 0.001884  eta: 0h12m  tot: 0h12m28s  (49.7%)48.6%  lr: 0.024795  loss: 0.001880  eta: 0h12m  tot: 0h12m29s  (49.7%)49.2%  lr: 0.024745  loss: 0.001876  eta: 0h12m  tot: 0h12m31s  (49.8%)49.2%  lr: 0.024735  loss: 0.001876  eta: 0h12m  tot: 0h12m31s  (49.8%)49.8%  lr: 0.024705  loss: 0.001873  eta: 0h12m  tot: 0h12m33s  (50.0%)  eta: 0h12m  tot: 0h12m34s  (50.0%)50.5%  lr: 0.024635  loss: 0.001874  eta: 0h12m  tot: 0h12m35s  (50.1%)52.1%  lr: 0.024465  loss: 0.001876  eta: 0h12m  tot: 0h12m39s  (50.4%)54.3%  lr: 0.024224  loss: 0.001865  eta: 0h12m  tot: 0h12m45s  (50.9%)54.5%  lr: 0.024224  loss: 0.001864  eta: 0h12m  tot: 0h12m46s  (50.9%)54.6%  lr: 0.024204  loss: 0.001863  eta: 0h12m  tot: 0h12m46s  (50.9%)54.9%  lr: 0.024184  loss: 0.001866  eta: 0h12m  tot: 0h12m47s  (51.0%)55.5%  lr: 0.024124  loss: 0.001873  eta: 0h12m  tot: 0h12m49s  (51.1%)55.6%  lr: 0.024094  loss: 0.001872  eta: 0h12m  tot: 0h12m49s  (51.1%)55.7%  lr: 0.024094  loss: 0.001872  eta: 0h12m  tot: 0h12m49s  (51.1%)55.8%  lr: 0.024094  loss: 0.001870  eta: 0h11m  tot: 0h12m50s  (51.2%)57.0%  lr: 0.023974  loss: 0.001878  eta: 0h11m  tot: 0h12m53s  (51.4%)57.9%  lr: 0.023924  loss: 0.001879  eta: 0h11m  tot: 0h12m56s  (51.6%)59.9%  lr: 0.023814  loss: 0.001884  eta: 0h11m  tot: 0h13m1s  (52.0%)60.6%  lr: 0.023754  loss: 0.001883  eta: 0h11m  tot: 0h13m3s  (52.1%)61.6%  lr: 0.023684  loss: 0.001889  eta: 0h11m  tot: 0h13m6s  (52.3%)61.8%  lr: 0.023684  loss: 0.001888  eta: 0h11m  tot: 0h13m7s  (52.4%)62.1%  lr: 0.023654  loss: 0.001889  eta: 0h11m  tot: 0h13m8s  (52.4%)64.5%  lr: 0.023434  loss: 0.001890  eta: 0h11m  tot: 0h13m15s  (52.9%)64.9%  lr: 0.023414  loss: 0.001889  eta: 0h11m  tot: 0h13m16s  (53.0%)65.4%  lr: 0.023353  loss: 0.001887  eta: 0h11m  tot: 0h13m17s  (53.1%)66.0%  lr: 0.023273  loss: 0.001884  eta: 0h11m  tot: 0h13m19s  (53.2%)66.6%  lr: 0.023243  loss: 0.001879  eta: 0h11m  tot: 0h13m20s  (53.3%)69.1%  lr: 0.023023  loss: 0.001864  eta: 0h11m  tot: 0h13m27s  (53.8%)69.9%  lr: 0.022973  loss: 0.001863  eta: 0h11m  tot: 0h13m30s  (54.0%)71.0%  lr: 0.022833  loss: 0.001868  eta: 0h11m  tot: 0h13m33s  (54.2%)72.1%  lr: 0.022663  loss: 0.001867  eta: 0h11m  tot: 0h13m36s  (54.4%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.020000  loss: 0.001893  eta: 0h9m  tot: 0h14m51s  (60.0%)72.6%  lr: 0.022623  loss: 0.001864  eta: 0h11m  tot: 0h13m37s  (54.5%)73.5%  lr: 0.022593  loss: 0.001865  eta: 0h11m  tot: 0h13m40s  (54.7%)74.9%  lr: 0.022423  loss: 0.001864  eta: 0h10m  tot: 0h13m44s  (55.0%)75.8%  lr: 0.022292  loss: 0.001869  eta: 0h10m  tot: 0h13m46s  (55.2%)78.1%  lr: 0.022042  loss: 0.001866  eta: 0h10m  tot: 0h13m53s  (55.6%)79.6%  lr: 0.021872  loss: 0.001866  eta: 0h10m  tot: 0h13m57s  (55.9%)80.2%  lr: 0.021792  loss: 0.001868  eta: 0h10m  tot: 0h13m59s  (56.0%)80.7%  lr: 0.021772  loss: 0.001869  eta: 0h10m  tot: 0h14m0s  (56.1%)81.3%  lr: 0.021742  loss: 0.001871  eta: 0h10m  tot: 0h14m2s  (56.3%)81.7%  lr: 0.021702  loss: 0.001872  eta: 0h10m  tot: 0h14m3s  (56.3%)82.2%  lr: 0.021622  loss: 0.001870  eta: 0h10m  tot: 0h14m4s  (56.4%)82.4%  lr: 0.021622  loss: 0.001870  eta: 0h10m  tot: 0h14m5s  (56.5%)85.0%  lr: 0.021372  loss: 0.001878  eta: 0h10m  tot: 0h14m12s  (57.0%)86.1%  lr: 0.021211  loss: 0.001881  eta: 0h10m  tot: 0h14m16s  (57.2%)86.8%  lr: 0.021151  loss: 0.001883  eta: 0h10m  tot: 0h14m18s  (57.4%)88.0%  lr: 0.021041  loss: 0.001885  eta: 0h10m  tot: 0h14m21s  (57.6%)%  lr: 0.021021  loss: 0.001883  eta: 0h10m  tot: 0h14m22s  (57.7%)88.6%  lr: 0.021021  loss: 0.001880  eta: 0h10m  tot: 0h14m23s  (57.7%)89.1%  lr: 0.020961  loss: 0.001881  eta: 0h10m  tot: 0h14m24s  (57.8%)90.3%  lr: 0.020871  loss: 0.001883  eta: 0h10m  tot: 0h14m28s  (58.1%)90.4%  lr: 0.020851  loss: 0.001884  eta: 0h10m  tot: 0h14m28s  (58.1%)91.5%  lr: 0.020721  loss: 0.001884  eta: 0h10m  tot: 0h14m32s  (58.3%)91.6%  lr: 0.020691  loss: 0.001883  eta: 0h10m  tot: 0h14m32s  (58.3%)92.6%  lr: 0.020611  loss: 0.001883  eta: 0h10m  tot: 0h14m35s  (58.5%)93.1%  lr: 0.020551  loss: 0.001882  eta: 0h10m  tot: 0h14m36s  (58.6%)93.8%  lr: 0.020471  loss: 0.001886  eta: 0h10m  tot: 0h14m38s  (58.8%)94.5%  lr: 0.020401  loss: 0.001885  eta: 0h9m  tot: 0h14m40s  (58.9%)94.9%  lr: 0.020340  loss: 0.001884  eta: 0h9m  tot: 0h14m41s  (59.0%)0.020340  loss: 0.001884  eta: 0h9m  tot: 0h14m42s  (59.0%)95.2%  lr: 0.020330  loss: 0.001885  eta: 0h9m  tot: 0h14m42s  (59.0%)96.2%  lr: 0.020200  loss: 0.001888  eta: 0h9m  tot: 0h14m45s  (59.2%)96.3%  lr: 0.020200  loss: 0.001888  eta: 0h9m  tot: 0h14m45s  (59.3%)96.4%  lr: 0.020200  loss: 0.001889  eta: 0h9m  tot: 0h14m46s  (59.3%)97.0%  lr: 0.020140  loss: 0.001891  eta: 0h9m  tot: 0h14m47s  (59.4%)97.0%  lr: 0.020140  loss: 0.001893  eta: 0h9m  tot: 0h14m47s  (59.4%)\n",
      " ---+++                Epoch    2 Train error : 0.00189439 +++--- ���\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 73.2%  lr: 0.012453  loss: 0.001632  eta: 0h5m  tot: 0h18m6s  (74.6%)).2%  lr: 0.019990  loss: 0.000795  eta: 0h8m  tot: 0h14m52s  (60.0%)0.8%  lr: 0.019960  loss: 0.001113  eta: 0h9m  tot: 0h14m54s  (60.2%)1.1%  lr: 0.019940  loss: 0.001204  eta: 0h9m  tot: 0h14m55s  (60.2%)%  lr: 0.019920  loss: 0.001238  eta: 0h9m  tot: 0h14m55s  (60.3%)1.5%  lr: 0.019910  loss: 0.001220  eta: 0h9m  tot: 0h14m56s  (60.3%)%  lr: 0.019850  loss: 0.001586  eta: 0h9m  tot: 0h14m57s  (60.4%)2.4%  lr: 0.019800  loss: 0.001557  eta: 0h9m  tot: 0h14m58s  (60.5%)3.1%  lr: 0.019750  loss: 0.001528  eta: 0h9m  tot: 0h15m0s  (60.6%)3.3%  lr: 0.019700  loss: 0.001507  eta: 0h9m  tot: 0h15m1s  (60.7%)3.9%  lr: 0.019680  loss: 0.001476  eta: 0h9m  tot: 0h15m3s  (60.8%)4.2%  lr: 0.019650  loss: 0.001493  eta: 0h9m  tot: 0h15m4s  (60.8%)4.3%  lr: 0.019650  loss: 0.001502  eta: 0h9m  tot: 0h15m4s  (60.9%)  lr: 0.019620  loss: 0.001551  eta: 0h9m  tot: 0h15m4s  (60.9%)5.9%  lr: 0.019510  loss: 0.001622  eta: 0h9m  tot: 0h15m8s  (61.2%)6.5%  lr: 0.019469  loss: 0.001627  eta: 0h9m  tot: 0h15m10s  (61.3%)7.1%  lr: 0.019389  loss: 0.001613  eta: 0h9m  tot: 0h15m12s  (61.4%)7.9%  lr: 0.019349  loss: 0.001596  eta: 0h9m  tot: 0h15m14s  (61.6%)10.3%  lr: 0.019109  loss: 0.001623  eta: 0h8m  tot: 0h15m20s  (62.1%)11.8%  lr: 0.018919  loss: 0.001606  eta: 0h8m  tot: 0h15m24s  (62.4%)%  lr: 0.018809  loss: 0.001606  eta: 0h8m  tot: 0h15m26s  (62.5%)13.6%  lr: 0.018719  loss: 0.001601  eta: 0h8m  tot: 0h15m28s  (62.7%)14.4%  lr: 0.018669  loss: 0.001578  eta: 0h8m  tot: 0h15m30s  (62.9%)%  lr: 0.018599  loss: 0.001554  eta: 0h8m  tot: 0h15m32s  (63.0%)15.9%  lr: 0.018499  loss: 0.001560  eta: 0h8m  tot: 0h15m34s  (63.2%)%  lr: 0.018489  loss: 0.001571  eta: 0h8m  tot: 0h15m35s  (63.2%)16.9%  lr: 0.018408  loss: 0.001554  eta: 0h8m  tot: 0h15m37s  (63.4%)17.7%  lr: 0.018378  loss: 0.001560  eta: 0h8m  tot: 0h15m39s  (63.5%)18.6%  lr: 0.018268  loss: 0.001549  eta: 0h8m  tot: 0h15m41s  (63.7%)%  lr: 0.018248  loss: 0.001559  eta: 0h8m  tot: 0h15m42s  (63.8%)19.4%  lr: 0.018238  loss: 0.001556  eta: 0h8m  tot: 0h15m43s  (63.9%)%  lr: 0.018188  loss: 0.001556  eta: 0h7m  tot: 0h15m44s  (64.0%)20.0%  lr: 0.018188  loss: 0.001558  eta: 0h7m  tot: 0h15m45s  (64.0%)20.2%  lr: 0.018178  loss: 0.001551  eta: 0h7m  tot: 0h15m45s  (64.0%)20.3%  lr: 0.018158  loss: 0.001556  eta: 0h7m  tot: 0h15m46s  (64.1%)22.3%  lr: 0.017968  loss: 0.001584  eta: 0h7m  tot: 0h15m51s  (64.5%)23.2%  lr: 0.017858  loss: 0.001588  eta: 0h7m  tot: 0h15m53s  (64.6%)23.6%  lr: 0.017828  loss: 0.001585  eta: 0h7m  tot: 0h15m54s  (64.7%)%  lr: 0.017798  loss: 0.001581  eta: 0h7m  tot: 0h15m55s  (64.8%)%  lr: 0.017768  loss: 0.001590  eta: 0h7m  tot: 0h15m55s  (64.8%)25.0%  lr: 0.017668  loss: 0.001577  eta: 0h7m  tot: 0h15m58s  (65.0%)25.7%  lr: 0.017648  loss: 0.001581  eta: 0h7m  tot: 0h15m59s  (65.1%)%  lr: 0.017578  loss: 0.001589  eta: 0h7m  tot: 0h16m1s  (65.3%)26.6%  lr: 0.017548  loss: 0.001587  eta: 0h7m  tot: 0h16m1s  (65.3%)27.4%  lr: 0.017498  loss: 0.001593  eta: 0h7m  tot: 0h16m4s  (65.5%)27.5%  lr: 0.017478  loss: 0.001589  eta: 0h7m  tot: 0h16m4s  (65.5%)27.9%  lr: 0.017417  loss: 0.001588  eta: 0h7m  tot: 0h16m5s  (65.6%)28.1%  lr: 0.017407  loss: 0.001587  eta: 0h7m  tot: 0h16m5s  (65.6%)28.6%  lr: 0.017297  loss: 0.001575  eta: 0h7m  tot: 0h16m7s  (65.7%)29.0%  lr: 0.017267  loss: 0.001578  eta: 0h7m  tot: 0h16m8s  (65.8%)29.4%  lr: 0.017217  loss: 0.001583  eta: 0h7m  tot: 0h16m9s  (65.9%)29.6%  lr: 0.017207  loss: 0.001582  eta: 0h7m  tot: 0h16m9s  (65.9%)  tot: 0h16m11s  (66.0%)31.2%  lr: 0.016977  loss: 0.001586  eta: 0h7m  tot: 0h16m13s  (66.2%)31.4%  lr: 0.016957  loss: 0.001588  eta: 0h7m  tot: 0h16m14s  (66.3%)32.1%  lr: 0.016857  loss: 0.001589  eta: 0h7m  tot: 0h16m16s  (66.4%)32.9%  lr: 0.016747  loss: 0.001589  eta: 0h7m  tot: 0h16m18s  (66.6%)%  lr: 0.016717  loss: 0.001587  eta: 0h7m  tot: 0h16m18s  (66.6%)33.3%  lr: 0.016697  loss: 0.001591  eta: 0h7m  tot: 0h16m19s  (66.7%)%  lr: 0.016667  loss: 0.001590  eta: 0h7m  tot: 0h16m20s  (66.7%)33.8%  lr: 0.016667  loss: 0.001589  eta: 0h7m  tot: 0h16m20s  (66.8%)35.0%  lr: 0.016527  loss: 0.001597  eta: 0h7m  tot: 0h16m23s  (67.0%)36.5%  lr: 0.016336  loss: 0.001598  eta: 0h7m  tot: 0h16m27s  (67.3%)37.3%  lr: 0.016276  loss: 0.001605  eta: 0h7m  tot: 0h16m29s  (67.5%)37.4%  lr: 0.016276  loss: 0.001617  eta: 0h7m  tot: 0h16m29s  (67.5%)%  lr: 0.016176  loss: 0.001620  eta: 0h7m  tot: 0h16m32s  (67.7%)39.8%  lr: 0.015956  loss: 0.001626  eta: 0h6m  tot: 0h16m35s  (68.0%)39.9%  lr: 0.015946  loss: 0.001625  eta: 0h6m  tot: 0h16m36s  (68.0%)40.2%  lr: 0.015936  loss: 0.001618  eta: 0h6m  tot: 0h16m36s  (68.0%)40.6%  lr: 0.015836  loss: 0.001625  eta: 0h6m  tot: 0h16m37s  (68.1%)40.9%  lr: 0.015806  loss: 0.001619  eta: 0h6m  tot: 0h16m38s  (68.2%)41.1%  lr: 0.015766  loss: 0.001619  eta: 0h6m  tot: 0h16m39s  (68.2%)42.0%  lr: 0.015706  loss: 0.001629  eta: 0h6m  tot: 0h16m41s  (68.4%)43.9%  lr: 0.015476  loss: 0.001623  eta: 0h6m  tot: 0h16m47s  (68.8%)44.2%  lr: 0.015466  loss: 0.001621  eta: 0h6m  tot: 0h16m48s  (68.8%)44.3%  lr: 0.015446  loss: 0.001619  eta: 0h6m  tot: 0h16m48s  (68.9%)44.9%  lr: 0.015385  loss: 0.001618  eta: 0h6m  tot: 0h16m50s  (69.0%)%  lr: 0.015385  loss: 0.001621  eta: 0h6m  tot: 0h16m50s  (69.0%)45.3%  lr: 0.015375  loss: 0.001625  eta: 0h6m  tot: 0h16m51s  (69.1%)47.4%  lr: 0.015175  loss: 0.001628  eta: 0h6m  tot: 0h16m57s  (69.5%)47.9%  lr: 0.015095  loss: 0.001631  eta: 0h6m  tot: 0h16m59s  (69.6%)48.1%  lr: 0.015075  loss: 0.001635  eta: 0h6m  tot: 0h16m59s  (69.6%)%  lr: 0.015035  loss: 0.001634  eta: 0h6m  tot: 0h17m0s  (69.7%)48.8%  lr: 0.014975  loss: 0.001633  eta: 0h6m  tot: 0h17m1s  (69.8%)49.2%  lr: 0.014945  loss: 0.001631  eta: 0h6m  tot: 0h17m3s  (69.8%)49.9%  lr: 0.014925  loss: 0.001628  eta: 0h6m  tot: 0h17m4s  (70.0%)50.1%  lr: 0.014905  loss: 0.001627  eta: 0h6m  tot: 0h17m5s  (70.0%)50.5%  lr: 0.014845  loss: 0.001626  eta: 0h6m  tot: 0h17m6s  (70.1%)51.9%  lr: 0.014775  loss: 0.001636  eta: 0h6m  tot: 0h17m10s  (70.4%)52.3%  lr: 0.014735  loss: 0.001633  eta: 0h6m  tot: 0h17m11s  (70.5%)52.6%  lr: 0.014705  loss: 0.001637  eta: 0h6m  tot: 0h17m11s  (70.5%)53.1%  lr: 0.014645  loss: 0.001640  eta: 0h6m  tot: 0h17m13s  (70.6%)55.5%  lr: 0.014435  loss: 0.001633  eta: 0h6m  tot: 0h17m19s  (71.1%)58.2%  lr: 0.014144  loss: 0.001636  eta: 0h6m  tot: 0h17m26s  (71.6%)58.8%  lr: 0.014114  loss: 0.001638  eta: 0h6m  tot: 0h17m28s  (71.8%)59.1%  lr: 0.014054  loss: 0.001636  eta: 0h6m  tot: 0h17m29s  (71.8%)59.4%  lr: 0.014014  loss: 0.001633  eta: 0h6m  tot: 0h17m30s  (71.9%)60.0%  lr: 0.013954  loss: 0.001632  eta: 0h6m  tot: 0h17m31s  (72.0%)60.5%  lr: 0.013934  loss: 0.001630  eta: 0h6m  tot: 0h17m33s  (72.1%)60.6%  lr: 0.013934  loss: 0.001631  eta: 0h6m  tot: 0h17m33s  (72.1%)62.5%  lr: 0.013644  loss: 0.001637  eta: 0h6m  tot: 0h17m38s  (72.5%)62.8%  lr: 0.013624  loss: 0.001636  eta: 0h6m  tot: 0h17m39s  (72.6%)63.6%  lr: 0.013534  loss: 0.001641  eta: 0h6m  tot: 0h17m41s  (72.7%)64.1%  lr: 0.013424  loss: 0.001641  eta: 0h6m  tot: 0h17m42s  (72.8%)64.5%  lr: 0.013393  loss: 0.001643  eta: 0h6m  tot: 0h17m43s  (72.9%)64.8%  lr: 0.013373  loss: 0.001640  eta: 0h5m  tot: 0h17m44s  (73.0%)65.0%  lr: 0.013373  loss: 0.001639  eta: 0h5m  tot: 0h17m44s  (73.0%)65.1%  lr: 0.013373  loss: 0.001639  eta: 0h5m  tot: 0h17m45s  (73.0%)66.8%  lr: 0.013133  loss: 0.001631  eta: 0h5m  tot: 0h17m49s  (73.4%)67.3%  lr: 0.013123  loss: 0.001631  eta: 0h5m  tot: 0h17m50s  (73.5%)67.6%  lr: 0.013083  loss: 0.001627  eta: 0h5m  tot: 0h17m51s  (73.5%)67.7%  lr: 0.013053  loss: 0.001628  eta: 0h5m  tot: 0h17m51s  (73.5%)68.1%  lr: 0.012953  loss: 0.001626  eta: 0h5m  tot: 0h17m53s  (73.6%)68.2%  lr: 0.012933  loss: 0.001625  eta: 0h5m  tot: 0h17m53s  (73.6%)68.4%  lr: 0.012903  loss: 0.001623  eta: 0h5m  tot: 0h17m53s  (73.7%)69.1%  lr: 0.012853  loss: 0.001625  eta: 0h5m  tot: 0h17m55s  (73.8%)70.6%  lr: 0.012693  loss: 0.001624  eta: 0h5m  tot: 0h17m59s  (74.1%)71.9%  lr: 0.012563  loss: 0.001625  eta: 0h5m  tot: 0h18m3s  (74.4%)72.4%  lr: 0.012523  loss: 0.001629  eta: 0h5m  tot: 0h18m4s  (74.5%)73.3%  lr: 0.012433  loss: 0.001632  eta: 0h5m  tot: 0h18m6s  (74.7%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.001598  eta: 0h4m  tot: 0h19m15s  (80.0%)5.1%  lr: 0.012302  loss: 0.001622  eta: 0h5m  tot: 0h18m11s  (75.0%)75.3%  lr: 0.012252  loss: 0.001620  eta: 0h5m  tot: 0h18m12s  (75.1%)77.2%  lr: 0.012142  loss: 0.001617  eta: 0h5m  tot: 0h18m17s  (75.4%)  lr: 0.012112  loss: 0.001619  eta: 0h5m  tot: 0h18m17s  (75.5%)77.6%  lr: 0.012092  loss: 0.001619  eta: 0h5m  tot: 0h18m18s  (75.5%)78.2%  lr: 0.012032  loss: 0.001618  eta: 0h5m  tot: 0h18m19s  (75.6%)78.9%  lr: 0.011952  loss: 0.001620  eta: 0h5m  tot: 0h18m21s  (75.8%)79.1%  lr: 0.011902  loss: 0.001620  eta: 0h5m  tot: 0h18m21s  (75.8%)81.0%  lr: 0.011762  loss: 0.001618  eta: 0h5m  tot: 0h18m27s  (76.2%)%  lr: 0.011752  loss: 0.001618  eta: 0h5m  tot: 0h18m27s  (76.2%)81.6%  lr: 0.011712  loss: 0.001614  eta: 0h5m  tot: 0h18m28s  (76.3%)82.4%  lr: 0.011642  loss: 0.001618  eta: 0h5m  tot: 0h18m30s  (76.5%)83.5%  lr: 0.011582  loss: 0.001614  eta: 0h5m  tot: 0h18m33s  (76.7%)83.7%  lr: 0.011552  loss: 0.001613  eta: 0h5m  tot: 0h18m34s  (76.7%)84.7%  lr: 0.011472  loss: 0.001610  eta: 0h5m  tot: 0h18m37s  (76.9%)86.7%  lr: 0.011221  loss: 0.001605  eta: 0h5m  tot: 0h18m42s  (77.3%)87.7%  lr: 0.011121  loss: 0.001609  eta: 0h4m  tot: 0h18m45s  (77.5%)87.8%  lr: 0.011091  loss: 0.001609  eta: 0h4m  tot: 0h18m45s  (77.6%)%  lr: 0.011001  loss: 0.001610  eta: 0h4m  tot: 0h18m48s  (77.8%)%  lr: 0.010901  loss: 0.001609  eta: 0h4m  tot: 0h18m50s  (77.9%)91.1%  lr: 0.010681  loss: 0.001609  eta: 0h4m  tot: 0h18m54s  (78.2%)93.5%  lr: 0.010411  loss: 0.001607  eta: 0h4m  tot: 0h19m1s  (78.7%)%  lr: 0.010371  loss: 0.001605  eta: 0h4m  tot: 0h19m3s  (78.8%)%  lr: 0.010371  loss: 0.001605  eta: 0h4m  tot: 0h19m3s  (78.9%)95.3%  lr: 0.010260  loss: 0.001604  eta: 0h4m  tot: 0h19m6s  (79.1%)0.010160  loss: 0.001603  eta: 0h4m  tot: 0h19m9s  (79.2%)97.2%  lr: 0.010070  loss: 0.001596  eta: 0h4m  tot: 0h19m12s  (79.4%)%  lr: 0.010030  loss: 0.001595  eta: 0h4m  tot: 0h19m13s  (79.6%)97.9%  lr: 0.010030  loss: 0.001595  eta: 0h4m  tot: 0h19m13s  (79.6%)\n",
      " ---+++                Epoch    3 Train error : 0.00154862 +++--- ���\n",
      "Training epoch 4: 0.01 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 80.6%  lr: 0.001802  loss: 0.001424  eta: <1min   tot: 0h23m0s  (96.1%)).4%  lr: 0.009980  loss: 0.001592  eta: 0h2m  tot: 0h19m16s  (80.1%)0.6%  lr: 0.009970  loss: 0.001517  eta: 0h3m  tot: 0h19m16s  (80.1%)0.9%  lr: 0.009950  loss: 0.001591  eta: 0h3m  tot: 0h19m17s  (80.2%)1.3%  lr: 0.009920  loss: 0.001473  eta: 0h3m  tot: 0h19m18s  (80.3%)1.5%  lr: 0.009890  loss: 0.001480  eta: 0h3m  tot: 0h19m19s  (80.3%)3.9%  lr: 0.009580  loss: 0.001429  eta: 0h4m  tot: 0h19m25s  (80.8%)4.1%  lr: 0.009520  loss: 0.001443  eta: 0h4m  tot: 0h19m26s  (80.8%)5.0%  lr: 0.009389  loss: 0.001482  eta: 0h4m  tot: 0h19m28s  (81.0%)5.7%  lr: 0.009359  loss: 0.001445  eta: 0h4m  tot: 0h19m30s  (81.1%)5.9%  lr: 0.009319  loss: 0.001421  eta: 0h4m  tot: 0h19m31s  (81.2%)6.7%  lr: 0.009269  loss: 0.001400  eta: 0h4m  tot: 0h19m33s  (81.3%)7.6%  lr: 0.009169  loss: 0.001403  eta: 0h4m  tot: 0h19m35s  (81.5%)8.5%  lr: 0.009109  loss: 0.001409  eta: 0h3m  tot: 0h19m37s  (81.7%)9.4%  lr: 0.009039  loss: 0.001410  eta: 0h3m  tot: 0h19m40s  (81.9%)9.5%  lr: 0.009029  loss: 0.001406  eta: 0h3m  tot: 0h19m40s  (81.9%)10.0%  lr: 0.008979  loss: 0.001397  eta: 0h3m  tot: 0h19m42s  (82.0%)10.3%  lr: 0.008929  loss: 0.001400  eta: 0h3m  tot: 0h19m42s  (82.1%)10.6%  lr: 0.008859  loss: 0.001388  eta: 0h3m  tot: 0h19m43s  (82.1%)11.3%  lr: 0.008799  loss: 0.001399  eta: 0h3m  tot: 0h19m45s  (82.3%)  lr: 0.008709  loss: 0.001411  eta: 0h3m  tot: 0h19m47s  (82.4%)%  lr: 0.008649  loss: 0.001417  eta: 0h3m  tot: 0h19m51s  (82.6%)13.5%  lr: 0.008609  loss: 0.001433  eta: 0h3m  tot: 0h19m52s  (82.7%)13.9%  lr: 0.008589  loss: 0.001434  eta: 0h3m  tot: 0h19m53s  (82.8%)17.3%  lr: 0.008218  loss: 0.001398  eta: 0h3m  tot: 0h20m3s  (83.5%)0.008178  loss: 0.001397  eta: 0h3m  tot: 0h20m3s  (83.5%)17.7%  lr: 0.008158  loss: 0.001394  eta: 0h3m  tot: 0h20m4s  (83.5%)%  lr: 0.008148  loss: 0.001396  eta: 0h3m  tot: 0h20m5s  (83.6%)18.5%  lr: 0.008128  loss: 0.001386  eta: 0h3m  tot: 0h20m6s  (83.7%)18.7%  lr: 0.008108  loss: 0.001392  eta: 0h3m  tot: 0h20m7s  (83.7%)%  lr: 0.008088  loss: 0.001401  eta: 0h3m  tot: 0h20m8s  (83.9%)20.6%  lr: 0.007978  loss: 0.001386  eta: 0h3m  tot: 0h20m12s  (84.1%)24.4%  lr: 0.007447  loss: 0.001376  eta: 0h3m  tot: 0h20m25s  (84.9%)24.5%  lr: 0.007427  loss: 0.001376  eta: 0h3m  tot: 0h20m25s  (84.9%)%  lr: 0.007377  loss: 0.001366  eta: 0h3m  tot: 0h20m27s  (85.0%)%  lr: 0.007367  loss: 0.001364  eta: 0h3m  tot: 0h20m28s  (85.1%)25.7%  lr: 0.007327  loss: 0.001362  eta: 0h3m  tot: 0h20m29s  (85.1%)25.8%  lr: 0.007317  loss: 0.001367  eta: 0h3m  tot: 0h20m29s  (85.2%)26.8%  lr: 0.007257  loss: 0.001377  eta: 0h3m  tot: 0h20m32s  (85.4%)27.9%  lr: 0.007127  loss: 0.001379  eta: 0h3m  tot: 0h20m34s  (85.6%)28.2%  lr: 0.007097  loss: 0.001379  eta: 0h3m  tot: 0h20m35s  (85.6%)28.8%  lr: 0.006987  loss: 0.001380  eta: 0h3m  tot: 0h20m37s  (85.8%)29.6%  lr: 0.006907  loss: 0.001379  eta: 0h3m  tot: 0h20m39s  (85.9%)31.1%  lr: 0.006797  loss: 0.001383  eta: 0h3m  tot: 0h20m44s  (86.2%)31.8%  lr: 0.006717  loss: 0.001379  eta: 0h3m  tot: 0h20m46s  (86.4%)32.0%  lr: 0.006697  loss: 0.001378  eta: 0h3m  tot: 0h20m47s  (86.4%)32.4%  lr: 0.006617  loss: 0.001372  eta: 0h3m  tot: 0h20m48s  (86.5%)32.6%  lr: 0.006597  loss: 0.001373  eta: 0h3m  tot: 0h20m48s  (86.5%)32.9%  lr: 0.006557  loss: 0.001375  eta: 0h3m  tot: 0h20m49s  (86.6%)34.3%  lr: 0.006416  loss: 0.001373  eta: 0h3m  tot: 0h20m53s  (86.9%)34.6%  lr: 0.006386  loss: 0.001373  eta: 0h3m  tot: 0h20m54s  (86.9%)35.3%  lr: 0.006326  loss: 0.001377  eta: 0h3m  tot: 0h20m56s  (87.1%)36.5%  lr: 0.006226  loss: 0.001386  eta: 0h3m  tot: 0h20m59s  (87.3%)37.1%  lr: 0.006186  loss: 0.001391  eta: 0h2m  tot: 0h21m0s  (87.4%)37.6%  lr: 0.006146  loss: 0.001394  eta: 0h2m  tot: 0h21m2s  (87.5%)38.3%  lr: 0.006106  loss: 0.001398  eta: 0h2m  tot: 0h21m4s  (87.7%)38.3%  lr: 0.006106  loss: 0.001398  eta: 0h2m  tot: 0h21m4s  (87.7%)38.4%  lr: 0.006086  loss: 0.001395  eta: 0h2m  tot: 0h21m4s  (87.7%)38.6%  lr: 0.006066  loss: 0.001394  eta: 0h2m  tot: 0h21m5s  (87.7%)39.2%  lr: 0.006016  loss: 0.001388  eta: 0h2m  tot: 0h21m6s  (87.8%)39.2%  lr: 0.006016  loss: 0.001389  eta: 0h2m  tot: 0h21m6s  (87.8%)39.5%  lr: 0.005986  loss: 0.001395  eta: 0h2m  tot: 0h21m7s  (87.9%)39.7%  lr: 0.005976  loss: 0.001394  eta: 0h2m  tot: 0h21m8s  (87.9%)40.1%  lr: 0.005946  loss: 0.001392  eta: 0h2m  tot: 0h21m9s  (88.0%)40.4%  lr: 0.005906  loss: 0.001387  eta: 0h2m  tot: 0h21m10s  (88.1%)41.3%  lr: 0.005796  loss: 0.001382  eta: 0h2m  tot: 0h21m12s  (88.3%)%  lr: 0.005776  loss: 0.001383  eta: 0h2m  tot: 0h21m12s  (88.3%)43.9%  lr: 0.005446  loss: 0.001403  eta: 0h2m  tot: 0h21m19s  (88.8%)44.4%  lr: 0.005405  loss: 0.001398  eta: 0h2m  tot: 0h21m20s  (88.9%)44.8%  lr: 0.005325  loss: 0.001399  eta: 0h2m  tot: 0h21m22s  (89.0%)%  lr: 0.005195  loss: 0.001389  eta: 0h2m  tot: 0h21m27s  (89.3%)%  lr: 0.005175  loss: 0.001390  eta: 0h2m  tot: 0h21m28s  (89.4%)48.9%  lr: 0.004945  loss: 0.001394  eta: 0h2m  tot: 0h21m33s  (89.8%)49.9%  lr: 0.004875  loss: 0.001396  eta: 0h2m  tot: 0h21m35s  (90.0%)50.1%  lr: 0.004875  loss: 0.001398  eta: 0h2m  tot: 0h21m36s  (90.0%)50.2%  lr: 0.004845  loss: 0.001397  eta: 0h2m  tot: 0h21m36s  (90.0%)50.9%  lr: 0.004785  loss: 0.001392  eta: 0h2m  tot: 0h21m38s  (90.2%)52.1%  lr: 0.004715  loss: 0.001384  eta: 0h2m  tot: 0h21m41s  (90.4%)52.8%  lr: 0.004645  loss: 0.001384  eta: 0h2m  tot: 0h21m43s  (90.6%)0.004645  loss: 0.001385  eta: 0h2m  tot: 0h21m43s  (90.6%)53.3%  lr: 0.004605  loss: 0.001384  eta: 0h2m  tot: 0h21m44s  (90.7%)54.7%  lr: 0.004435  loss: 0.001388  eta: 0h2m  tot: 0h21m48s  (90.9%)%  lr: 0.004425  loss: 0.001389  eta: 0h2m  tot: 0h21m48s  (91.0%)55.3%  lr: 0.004404  loss: 0.001388  eta: 0h2m  tot: 0h21m49s  (91.1%)55.4%  lr: 0.004394  loss: 0.001388  eta: 0h2m  tot: 0h21m50s  (91.1%)56.4%  lr: 0.004334  loss: 0.001395  eta: 0h2m  tot: 0h21m52s  (91.3%)56.9%  lr: 0.004284  loss: 0.001394  eta: 0h2m  tot: 0h21m54s  (91.4%)57.3%  lr: 0.004244  loss: 0.001399  eta: 0h1m  tot: 0h21m55s  (91.5%)58.9%  lr: 0.004104  loss: 0.001404  eta: 0h1m  tot: 0h21m59s  (91.8%)60.0%  lr: 0.003974  loss: 0.001406  eta: 0h1m  tot: 0h22m2s  (92.0%)60.5%  lr: 0.003934  loss: 0.001409  eta: 0h1m  tot: 0h22m3s  (92.1%)%  lr: 0.003924  loss: 0.001410  eta: 0h1m  tot: 0h22m4s  (92.2%)61.4%  lr: 0.003904  loss: 0.001410  eta: 0h1m  tot: 0h22m5s  (92.3%)62.1%  lr: 0.003814  loss: 0.001409  eta: 0h1m  tot: 0h22m7s  (92.4%)63.1%  lr: 0.003714  loss: 0.001406  eta: 0h1m  tot: 0h22m10s  (92.6%)63.6%  lr: 0.003664  loss: 0.001406  eta: 0h1m  tot: 0h22m11s  (92.7%)65.3%  lr: 0.003464  loss: 0.001407  eta: 0h1m  tot: 0h22m15s  (93.1%)%  lr: 0.003313  loss: 0.001414  eta: 0h1m  tot: 0h22m19s  (93.4%)67.0%  lr: 0.003273  loss: 0.001414  eta: 0h1m  tot: 0h22m20s  (93.4%)67.1%  lr: 0.003243  loss: 0.001414  eta: 0h1m  tot: 0h22m20s  (93.4%)67.2%  lr: 0.003233  loss: 0.001414  eta: 0h1m  tot: 0h22m20s  (93.4%)67.3%  lr: 0.003203  loss: 0.001413  eta: 0h1m  tot: 0h22m21s  (93.5%)68.1%  lr: 0.003143  loss: 0.001415  eta: 0h1m  tot: 0h22m23s  (93.6%)71.1%  lr: 0.002873  loss: 0.001416  eta: 0h1m  tot: 0h22m32s  (94.2%)%  lr: 0.002783  loss: 0.001416  eta: 0h1m  tot: 0h22m33s  (94.3%)%  lr: 0.002743  loss: 0.001415  eta: 0h1m  tot: 0h22m35s  (94.5%)73.5%  lr: 0.002653  loss: 0.001414  eta: 0h1m  tot: 0h22m39s  (94.7%)73.5%  lr: 0.002643  loss: 0.001415  eta: 0h1m  tot: 0h22m39s  (94.7%)74.9%  lr: 0.002503  loss: 0.001415  eta: 0h1m  tot: 0h22m43s  (95.0%)75.0%  lr: 0.002483  loss: 0.001416  eta: 0h1m  tot: 0h22m43s  (95.0%)75.4%  lr: 0.002433  loss: 0.001413  eta: 0h1m  tot: 0h22m45s  (95.1%)77.0%  lr: 0.002232  loss: 0.001418  eta: 0h1m  tot: 0h22m49s  (95.4%)77.6%  lr: 0.002202  loss: 0.001419  eta: 0h1m  tot: 0h22m51s  (95.5%)0.001417  eta: 0h1m  tot: 0h22m52s  (95.6%)78.3%  lr: 0.002112  loss: 0.001418  eta: 0h1m  tot: 0h22m54s  (95.7%)78.4%  lr: 0.002102  loss: 0.001418  eta: 0h1m  tot: 0h22m54s  (95.7%)79.7%  lr: 0.001952  loss: 0.001423  eta: <1min   tot: 0h22m57s  (95.9%)80.1%  lr: 0.001872  loss: 0.001423  eta: <1min   tot: 0h22m59s  (96.0%)80.7%  lr: 0.001792  loss: 0.001423  eta: <1min   tot: 0h23m0s  (96.1%)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.000000  loss: 0.001418  eta: <1min   tot: 0h23m49s  (100.0%)9%  lr: 0.001712  loss: 0.001417  eta: <1min   tot: 0h23m4s  (96.4%)82.1%  lr: 0.001702  loss: 0.001417  eta: <1min   tot: 0h23m5s  (96.4%)82.2%  lr: 0.001682  loss: 0.001417  eta: <1min   tot: 0h23m5s  (96.4%)%  lr: 0.001672  loss: 0.001417  eta: <1min   tot: 0h23m5s  (96.5%)84.3%  lr: 0.001462  loss: 0.001420  eta: <1min   tot: 0h23m11s  (96.9%)84.4%  lr: 0.001452  loss: 0.001419  eta: <1min   tot: 0h23m11s  (96.9%)85.4%  lr: 0.001411  loss: 0.001420  eta: <1min   tot: 0h23m14s  (97.1%)86.5%  lr: 0.001321  loss: 0.001424  eta: <1min   tot: 0h23m17s  (97.3%)86.9%  lr: 0.001301  loss: 0.001422  eta: <1min   tot: 0h23m18s  (97.4%)88.5%  lr: 0.001141  loss: 0.001422  eta: <1min   tot: 0h23m22s  (97.7%)88.6%  lr: 0.001131  loss: 0.001422  eta: <1min   tot: 0h23m22s  (97.7%)88.8%  lr: 0.001101  loss: 0.001422  eta: <1min   tot: 0h23m23s  (97.8%)89.4%  lr: 0.001021  loss: 0.001422  eta: <1min   tot: 0h23m25s  (97.9%)89.5%  lr: 0.000991  loss: 0.001421  eta: <1min   tot: 0h23m25s  (97.9%)90.4%  lr: 0.000861  loss: 0.001418  eta: <1min   tot: 0h23m28s  (98.1%)91.6%  lr: 0.000801  loss: 0.001416  eta: <1min   tot: 0h23m31s  (98.3%)%  lr: 0.000711  loss: 0.001418  eta: <1min   tot: 0h23m33s  (98.5%)%  lr: 0.000651  loss: 0.001416  eta: <1min   tot: 0h23m35s  (98.6%)93.3%  lr: 0.000571  loss: 0.001419  eta: <1min   tot: 0h23m36s  (98.7%)93.5%  lr: 0.000551  loss: 0.001419  eta: <1min   tot: 0h23m37s  (98.7%)93.6%  lr: 0.000541  loss: 0.001420  eta: <1min   tot: 0h23m37s  (98.7%)94.3%  lr: 0.000441  loss: 0.001421  eta: <1min   tot: 0h23m39s  (98.9%)94.7%  lr: 0.000360  loss: 0.001421  eta: <1min   tot: 0h23m40s  (98.9%)95.9%  lr: 0.000230  loss: 0.001419  eta: <1min   tot: 0h23m43s  (99.2%)98.0%  lr: 0.000060  loss: 0.001414  eta: <1min   tot: 0h23m48s  (99.6%)98.5%  lr: 0.000020  loss: 0.001415  eta: <1min   tot: 0h23m48s  (99.7%)98.7%  lr: 0.000010  loss: 0.001416  eta: <1min   tot: 0h23m48s  (99.7%)\n",
      " ---+++                Epoch    4 Train error : 0.00138226 +++--- ���\n",
      "Saving model to file : starspace_embedding\n",
      "Saving model in tsv format : starspace_embedding.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!starspace train -trainFile \"data/train_processed.tsv\" -model starspace_embedding \\\n",
    "-trainMode 3 -adagrad true -ngrams 1 -epoch 5 -dim 100 -similarity cosine -minCount 2 \\\n",
    "-verbose true -fileFormat labelDoc -negSearchLimit 10 -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### YOUR CODE HERE #############\n",
    "starspace_embeddings = dict()\n",
    "with open('starspace_embedding.tsv', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        row = line.strip().split('\\t')\n",
    "        starspace_embeddings[row[0]] = np.array(row[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.517 | Hits@   1: 0.517\n",
      "DCG@   5: 0.614 | Hits@   5: 0.697\n",
      "DCG@  10: 0.633 | Hits@  10: 0.756\n",
      "DCG@ 100: 0.664 | Hits@ 100: 0.905\n",
      "DCG@ 500: 0.674 | Hits@ 500: 0.982\n",
      "DCG@1000: 0.676 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current answer for task StarSpaceRanks is: 84\t54\t73\t80\t34\t86\t99\t66\t89\t25\t47\t77\t14\t20\t62\t67\t23\t8\t36\t87\t41\t18\t2\t46\t69\t68\t88\t85\t81\t4\t42\t43\t29\t72\t5...\n"
     ]
    }
   ],
   "source": [
    "starspace_ranks_results = []\n",
    "######### YOUR CODE HERE #############\n",
    "prepared_test_data = 'data/test_processed.tsv'\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You want to submit these parts:\n",
      "Task Question2Vec: 0.019293891266\n",
      "-0.0287272129208\n",
      "0.0460561104119\n",
      "0.0852593332529\n",
      "0.0243055559695\n",
      "-0.0729031041265\n",
      "0.0...\n",
      "Task HitsCount: 1.0\n",
      "0.5\n",
      "1.0\n",
      "0.5\n",
      "1.0\n",
      "0.333333333333\n",
      "0.666666666667\n",
      "1.0\n",
      "0.1\n",
      "0.2\n",
      "0.3\n",
      "0.4\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "0.1\n",
      "0....\n",
      "Task DCGScore: 1.0\n",
      "0.5\n",
      "0.815464876786\n",
      "0.5\n",
      "0.815464876786\n",
      "0.333333333333\n",
      "0.54364325119\n",
      "0.710309917857\n",
      "0.1\n",
      "0.16309297...\n",
      "Task W2VTokenizedRanks: 95\t94\t7\t9\t64\t36\t31\t93\t23\t100\t99\t20\t60\t6\t97\t48\t70\t37\t41\t96\t29\t56\t2\t65\t68\t44\t27\t25\t57\t62\t11\t87\t50\t66\t7...\n",
      "Task StarSpaceRanks: 84\t54\t73\t80\t34\t86\t99\t66\t89\t25\t47\t77\t14\t20\t62\t67\t23\t8\t36\t87\t41\t18\t2\t46\t69\t68\t88\t85\t81\t4\t42\t43\t29\t72\t5...\n"
     ]
    }
   ],
   "source": [
    "# EMAIL \n",
    "STUDENT_EMAIL = \"guipetor@gmail.com\"\n",
    "# TOKEN\n",
    "STUDENT_TOKEN = \"ojVklc3EBbktJZia\" \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted to Coursera platform. See results on assignment page!\n"
     ]
    }
   ],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
